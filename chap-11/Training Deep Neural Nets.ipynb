{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter we introduced artificial neural networks and trained our first deep neural network. It was a very shallow DNN, with only two hidden layers. What if we need to tackle more complex problems? Say about 10 layers? This would not be a walk in the park...\n",
    "\n",
    "- You would be faced with the _vanishing gradients_ problem that affects DNNs and makes lower layers hard to train.\n",
    "- With such a large network, training would be extremely slow...\n",
    "- A model with milions of parameters would severly risk overfitting the training set.\n",
    "\n",
    "In this chapter we will go over techniques on how to solve these problems so you too can be a Deep Learning master!\n",
    "\n",
    "## Vanishing/Exploding Gradients Problems\n",
    "\n",
    "As discussed in the previous chapter, the backpropigation algo works going from the output layer to the input. Once the algo has computed the gradients for each param, it uses these to update the parameter with a Gradient Descent step.\n",
    "\n",
    "Unfortunately, gradients get smaller and smaller as the algo progresses down to the lower layers. As a result, the GD update leaves the lower layer connection virtually unchanged. This is called the _vanishing gradients_ problem. In some cases, the gradients grow bigger and bigger, so many layers get huge weight updates and the algo diverges. This is the _exploding gradients_ problem, which is encountered mostly in recurrent neural networks. Generally, DNNs suffer from unstable gradients; different layers may learn at widely different speeds.\n",
    "\n",
    "Although this behavior has been observed for a great while, it's only in 2010 when significant progress was made. Read the book to see more, but in short they found that logistic sigmoid activation along with random initialization with std deviation of 1 and mean of 0.\n",
    "\n",
    "If you look at the logistic activation function, you can see that when the input becomes large, the function saturates to 0 or 1, and that kills propigation.\n",
    "\n",
    "## Xavier and He Initialization\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
