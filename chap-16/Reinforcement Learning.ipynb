{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reinforcement Learning_(RL) is currently one of the most popular fields of ML. It's been around since the 1950's, but never gained much headlines until 2013 when a startup called Deep Mind made the lines with a system that could play any Atari game from scratch! Another feat was achived when in March 2016 their system, AlphaGo, beat Lee Sedol, a Go master.\n",
    "\n",
    "They did it using some important techniques that will be coverd later in this chapter: _policy gradients_ and _deep Q-networks_(DQN), and _Markov decision processes_(MDP).\n",
    "\n",
    "## Learning to Optimize Rewards\n",
    "\n",
    "In RL, a software _agent_ makes _observations_ and takes _actions_ within an _environment_, and in return recieves _rewards_. In short, it's objective is to learn to act in a way to maximize long-term rewards.\n",
    "\n",
    "Some examples are:\n",
    "- The agent controlling a walking robot\n",
    "- The agent controlling pacman\n",
    "- The agent playing Go\n",
    "- The agent can control a thermomitor\n",
    "- The agent can observe stock prices\n",
    "\n",
    "Note that not every implementation needs to have a positive reward! Navigating a maze is a good example.\n",
    "\n",
    "## Policy Search\n",
    "\n",
    "The algorithm used by the software agent is it's _policy_. The policy can be any algorithm and it doesn't even have to be deterministic.\n",
    "\n",
    "Assume you have a robot vacuum cleaner who's reward is the amount of dust it picks up in 30 min. Its policy could be move forward with probability _p_ every second, or randomly rotate with probability 1 - _p_. The rotation angle would be a random angle between -r and +r. Since this involves randomness this would be a _stochastic policy_.\n",
    "\n",
    "How would you train this robot? There are only two _policy parameters_ to tweak: the probability _p_ and the range _r_. One algo is to try different  values for them and pick the one that works best. This is what's called _policy search_. However, when the _policy space_ is too large, finding a good set of parameters becomes unreasonable.\n",
    "\n",
    "Another way to explore this is to use _genetic algorithms_. You can create 100 policies and test them. Then kill off the 80 worst and make the 20 survivors produce 4 offspring each. An offspring is a copy of the parent with some random variation. This continues until a good policy is found.\n",
    "\n",
    "Yet another approach is to use optimization techniques! By evaluating the rewards with regards to the policy params, you can tweak them ot higher rewards (_gradient ascent_). This approach is called _policy gradients_(PG).\n",
    "\n",
    "Now, let's move on to creating an environment.\n",
    "\n",
    "## Introduction to OpenAI Gym\n",
    "\n",
    "One of the greatest challanges of RL is the environment. If you want an agent to play an Atari game, you need an emulator. If you want a walking robot, then you need to train it in the real world. You can't undo in the real world though, so you generally need a _simulatied environment_ to bootstrap training.\n",
    "\n",
    "Here's a quick gym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<CartPoleEnv<CartPole-v0>>>\n",
      "[ 0.00440016 -0.01125002  0.01610391 -0.01434425]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "print(env)\n",
    "\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want `render()` to return a NumPy array, you can set the **mode** param to **rgb_array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render(mode=\"rgb_array\")\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the env what actions are possible..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two discrete actions, 0 and 1, which represent accelerating left or right respectively.\n",
    "\n",
    "Lets try accelerating to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00417516  0.18363732  0.01581702 -0.30190301]\n",
      "1.0\n",
      "False\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(obs)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step method executes and returns a given action:\n",
    "\n",
    "obs\n",
    "> The new observation/state.\n",
    "\n",
    "reward\n",
    "> In this environment, you get a reward of 1.0 every step\n",
    "\n",
    "done\n",
    "> This will be true when the _episode_ is over.\n",
    "\n",
    "info\n",
    "> Dictionary may provide debug information. This should not be used for training.\n",
    "\n",
    "Let's hardcode a simple policy that accelerates left wehn the pole is leaning left and right when it leans right. We'll run this and see the average reward it gets over 500 episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(1000):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.35 8.87780941448959 25.0 68.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean(totals), np.std(totals), np.min(totals), np.max(totals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It wasn't able to hold itself up for longer than 68 steps! Big OOF.\n",
    "\n",
    "## Neural Network Policies\n",
    "\n",
    "We're going to create neural network policy. This will take an observation as the input and output a probability for each action, and then select an action randomly accorrding to the probabilites. It will output probabilit _p_ of action 0 (left), and the probability of action 1 (right) will be 1 - _p_.\n",
    "\n",
    "A good question to ask here is why are we picking a random action based on probability? This lets the agent find the right balance between _exploring_ and _exploiting_. Think of the restaurant problem!\n",
    "\n",
    "Do note that in this environment, the past state is not needed! The CartPole problem is as simple as can be! Here's the code for this policiy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "# 1. Specify the neural network architecture\n",
    "n_inputs = 4\n",
    "n_hidden = 4\n",
    "n_outputs = 1\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "# 2. Build the neural network\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu,\n",
    "                        weights_initializer=initializer)\n",
    "logits = fully_connected(hidden, n_outputs, activation_fn=None,\n",
    "                        weights_initializer=initializer)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "# 3. Select a random action based on the estimated probabilities\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A breif run through of the code above.\n",
    "\n",
    "1. We define the architecture. 4 inputs from the CartPole, 4 hidden units.\n",
    "\n",
    "2. Next we build the neural network. This is a vanilla MLP here. Note the output uses the logistic activation function for an ouput from 0.0 to 1.0. If there were more than two actions, there would be one neuron per action and softmax activation would be used instead.\n",
    "\n",
    "3. Lastly, the `multinomial()` function is used to pick a random action.\n",
    "\n",
    "Now we have a neural network policy... but how do we train it?\n",
    "\n",
    "## Evaluating Actions: The Credit Assignment Problem\n",
    "\n",
    "If we knew the best actions at each step, we could just train the network by minimizing the cross entropy between the estimated and target probability. That would be supervised learning however, and in RL the only guidence the agent gets in through rewards, and those are typically spare and delayed. For example, an agent balances a pole for 100 steps... how can it now which of them were bad or good? They know the last action is where the pole fell, but is not necesarally what caused it. This is called the _credit assignment problem_: when an agent gets a reward, it's hard to know what should be credited or blamed.\n",
    "\n",
    "The common strategy for this is to evaluate the action based on the sum of all rewards that come after it, after applying a _discount rate r_ at each step. Discount rates are typically 0.95 or 0.99, with 13 steps roughly halfing the rewards of 0.95 and 69 steps half the rewards for 0.99.\n",
    "\n",
    "Using this model, on average good actions will get a better score than bad ones. To get fairly reliable scores, we must run many episodes and normalize all the action scores. Now that we can evaluate each action, we can train our first agent using policy gradients.\n",
    "\n",
    "## Policy Gradients\n",
    "\n",
    "PG algos optimize the params by following the gradients toward higher rewards. One class of algos, call _REINFORCE algorithms_, was introduced in 1992. Here's a common variant:\n",
    "\n",
    "1. First, let the neural network play the game several times. At each step compute what would make the chosen action even more likely, but don't apply the gradients yet.\n",
    "\n",
    "2. Once you've run several episodes, compute each action's score.\n",
    "\n",
    "3. If an actions score is positive, apply the gradients computed earlier. If negative, apply the opposite gradients to make less likely.\n",
    "\n",
    "4. Finally, compute the mean of all the resulting gradient vectors and use it to perform a Gradient Descent step.\n",
    "\n",
    "Let's implement this algo to train our neural network policy. Let's start by adding the target probability, the cost function, and the training operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1.0 - tf.to_float(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a target proba, we can define a cost function and compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y,\n",
    "                                                       logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars =optimizer.compute_gradients(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we call `compute_gradients()` and not `minimize()`. This is because we want to tweak them before we apply them.\n",
    "\n",
    "Lets put all the gradients in a list for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = [grad for grad, variable in grads_and_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to save the computed gradients for each action at each step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "    \n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an initializer and saver!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to the execution phase! We need a couple of funcs to compute the total discounted rewards, given the raw rewards, and to normalize the results across multiple episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                             for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std\n",
    "           for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to check that it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-22. -40. -50.]\n"
     ]
    }
   ],
   "source": [
    "res = discount_rewards([10, 0, -50], discount_rate=0.8)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]\n"
     ]
    }
   ],
   "source": [
    "res = discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After verifying the two funcitions, all we have to do is train the policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a22a9cbae9a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mcurrent_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mcurrent_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ML-Practice-MSFQzO5J/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ML-Practice-MSFQzO5J/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ML-Practice-MSFQzO5J/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ML-Practice-MSFQzO5J/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactive\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \"\"\"\n\u001b[0;32m-> 1228\u001b[0;31m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_COLOR_BUFFER_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_DEPTH_BUFFER_BIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ML-Practice-MSFQzO5J/lib/python3.6/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iterations = 250\n",
    "n_max_steps = 1000\n",
    "n_games_per_update = 10\n",
    "save_iterations = 10\n",
    "discount_rate = 0.95\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run(\n",
    "                [action, gradients],\n",
    "                feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "                obs, reward, done, info = env.step(action_val[0][0])\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                env.render() #fun\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "        \n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
    "        feed_dict = {}\n",
    "        for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean(\n",
    "            [reward * all_gradients[game_index][step][var_index]\n",
    "            for game_index, rewards in enumerate(all_rewards)\n",
    "            for step, reward in enumerate(rewards)],\n",
    "            axis=0)\n",
    "            feed_dict[grad_placeholder] = mean_gradients\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "            saver.save(sess, \"./my_policy_net_pg.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the relative simplicity, this algo is quite powerful! In fact, AlphaGo was based on a similar PG algo (plus _Monte Carlo Tree Search_, which you should look up!).\n",
    "\n",
    "We'll now look at another popular familly of algos. Whereas PG directly try and optimize the policy rewards, the agent will instead estimate the sum of expected sum of discounted future rewards for each state. To understand these algos, we need to understand _Markov decision processes_(MDP).\n",
    "\n",
    "## Markov Decision Processes\n",
    "\n",
    "In the early 20th century, Andrey Markov studied stochastic processes with no memory called _Markov chains_. It had fixed states and randomly evolves from one state to another at each step.  The probability to evolve from _s_ to _s'_ is fixed.\n",
    "\n",
    "Markov decision processes resemble Markov chains but with a twist: at each step the agent can chose  on of several actions, and the transition properties depended on the action. Moreover, some state transitions return some reward with the agents goal being to maximize said reward.\n",
    "\n",
    "Refer to book for more examples.\n",
    "\n",
    "Bellman found a way to estimate the _optimal state value_ of any state _s_, this is the sum of all discounted future rewards the agent can expect on average after reaching state _s_, assuming it acts optimally.\n",
    "\n",
    "Refer to book for equation.\n",
    "\n",
    "This leads to an algo that can predict the optimal state value of every state: you first initialize the state value estimates to zero, and then you iteratively update them using the _Value Iteration_ algo. Given enough time, the estimates will converge to the optimal state values.\n",
    "\n",
    "This is great in policy evaluation, but this doesn't tell the agent what to do. Bellman found a similar algo ot estimate the optimal _state-action values_ generally called _Q-Values_. The optimal Q-Value is the sum of discounted future rewards the agent can expect on average, assuming it acts optimally.\n",
    "\n",
    "Once the values have been found, implementing them is trivial. When in state _s_, it should choose the action with the highest Q-Value for that state. We're going to apply this to the MDP shown in the book. First, to define the MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = np.nan\n",
    "T = np.array([\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\n",
    "    [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]]\n",
    "])\n",
    "R = np.array([\n",
    "    [[10.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "    [[10.0, 0.0, 0.0], [nan, nan ,nan], [0.0, 0.0, -50.0]],\n",
    "    [[nan, nan, nan], [40.0, 0.0, 0.0], [nan, nan, nan]],\n",
    "])\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the Q-Value Interation algo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0\n",
    "    \n",
    "learning_rate = 0.01\n",
    "discount_rate = 0.95\n",
    "n_iterations = 100\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    Q_prev = Q.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q[s, a] = np.sum([\n",
    "                T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q)\n",
    "print(np.argmax(Q, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the optimal policy for this MDP, with a discound of 0.95. If the rate were 0.9, in state S1 the best action becomes a0 (stay put) because it values short term gain much more than long term gain.\n",
    "\n",
    "## Temporal Difference Learning and Q-Learning\n",
    "\n",
    "RL problems can typically be modeled into MDPs, but the agent does not know what the rewards are going to be. This means each state and transition must be expirenced at least once to know the rewards, and must be completed multiple times to have a reasonable estimate.\n",
    "\n",
    "The _Temporal Difference Learning_(TD Learning) algorithm is simialy to the Value Iteration algo but tweaked to take into acount that the agent has partial knowledge if the MDP. We assume that the agent initially knows only the possible states and actions. The agent uses an _exploration policy_ to explore the MDP and updates the estimates of the state values based on the transitions and rewards observed.\n",
    "\n",
    "> A quick side note, TD Learning is much like Stochastic Gradient Descent in that it handles one sample at a time. It can only truely converge if you gradually reduce the learning rate.\n",
    "\n",
    "For each state _s_, this algo keeps track of the running average the agent gets upon leaving the state, plus the reward it expects to get later (assuming optimal actions).\n",
    "\n",
    "The Q-Learning algo is an adaptaition of the Q-Value Iter. algo, where the transition probabilities and rewards are initially unkown.\n",
    "\n",
    "For each state-action pair(_s_, _a_), this algo keeps track of the running average of the rewards the agent gets upon leaving the state _s_ with action _a_, plus the rewards it expects later. Since the target policy would act optimally, we take the max of the Q-Value estimates for the next state.\n",
    "\n",
    "Next is now to implement Q-Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rnd\n",
    "\n",
    "learning_rate0 = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iteration = 20000\n",
    "\n",
    "s = 0 # Start in state 0\n",
    "\n",
    "Q = np.full((3, 3), -np.inf)  # -inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0 # Inital value = 0.0 for all possible actions\n",
    "    \n",
    "for iteration in range(n_iterations):\n",
    "    a = rnd.choice(possible_actions[s]) # choose an action (randomly)\n",
    "    sp = rnd.choice(range(3), p=T[s,a]) # pick next state using T[s,a]\n",
    "    reward = R[s, a, sp]\n",
    "    learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)\n",
    "    Q[s, a] = learning_rate * Q[s, a] + (1 - learning_rate) * (\n",
    "    reward + discount_rate * np.max(Q[sp])\n",
    "    )\n",
    "    s = sp # Move to the next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given enough iterations, this would converge to the optimal Q-Values. This is called an _off-policy_ algorithm because the policy trained is not the one being executed.\n",
    "\n",
    "## Exploration Policies\n",
    "\n",
    "Q-Learning can work only if the exploration of the MDP was thourgh enough. While a purely random policy would eventually visit every state, and transition many times, it would take an extremly long time. Therefore, the better option is to use the _ε-greedy policy_: at each step it acts randomly with a probability ε, or greedily with probability 1-ε. The advantage of this policy is that both the interesting and unknown parts of the MDP are explored. It's quite common to start with a high value and then gradually reduce it (e.g., 1.0 -> 0.05).\n",
    "\n",
    "Or, rather than relying on chance for exploration, you can encourage the exploration policy to try actions it hasn't tried much before. This can be implemented by a bonus added to the Q-Value estimates!\n",
    "\n",
    "## Approximate Q-Learning\n",
    "\n",
    "One of the great problems with Q-Learning is that it does't scale well to large (or even medium) MDPs with many states and actions. For example Ms. Pac-Man has 250 pellets, each having two states, making a total of 2^250 states for pellets alone! More than the number of atoms in the observable universe!\n",
    "\n",
    "The solution is to approximate the Q-Values using a manageable number of parameters. This is called _Approximate Q-Learning_. DeepMind showed that using DNNs can work rather well, especially for complex problems. A DNN used to estimate Q-Values is called a _deep Q-network_(DQN), using a DQN for Approximate Q-Learning is called _Deep Q-Learning_.\n",
    "\n",
    "We'll now use a Deep Q-Learning to train an agent to play Ms. Pac-Man. The code can be tweak to play most Atari Games, but it works best at action games.\n",
    "\n",
    "## Learning to Play Ms. Pac-Man Using Deep Q-Learning\n",
    "\n",
    "First, we make the Ms. Pac-Man environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "Discrete(9)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "print(obs.shape) # [height, width, channels]\n",
    "print(env.action_space) # Valid Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the actions are simple the 8 degrees with the last action being center stick. The Obs is just the RGB of the screen. They're a bit large, so we'll scale it down..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mspacman_color = np.array([210, 164, 74]).mean()\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # crop and downsize\n",
    "    img = img.mean(axis=2) # to greyscale\n",
    "    img[img==mspacman_color] = 0 # improve contrast\n",
    "    img = (img - 128) / 128 - 1 # normalize from -1.0 to 1.0\n",
    "    return img.reshape(88, 80, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the DQN. It could just take a state-action pair, but since the actions are descrete, it's more convinient to use a neural network that takes only a state _s_ as input and oputs one Q-Value per action. The DQN will be composed of three Convolutional layers, followed by two fully connected layers, including the output.\n",
    "\n",
    "The training algo we use requires two DQNs with the same architecture. On to drive Ms. Pac-Man during training (the _actor_), the other to watch the actor and learn from its trials and errors (the _critic_). At regular intervals, the critic will be copied to the actor. Since we need two DQNs, we'll create a function to build them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import convolution2d, fully_connected\n",
    "\n",
    "learning_rate = 0.01\n",
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32, 64, 64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_paddings = [\"SAME\"]*3\n",
    "conv_activation = [tf.nn.relu]*3\n",
    "n_hidden_in = 64 * 11 * 10 # conv3 has 64 maps of 11x10 each\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n # 9 discrete actions!\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def q_network(X_state, scope):\n",
    "    prev_layer = X_state\n",
    "    conv_layers = []\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        for n_maps, kernel_size, stride, padding, activation in zip(\n",
    "                conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "                conv_paddings, conv_activation):\n",
    "            prev_layer = convolution2d(\n",
    "                prev_layer, num_outputs=n_maps, kernel_size=kernel_size,\n",
    "                stride=stride, padding=padding, activation_fn=activation,\n",
    "                weights_initializer=initializer)\n",
    "            conv_layers.append(prev_layer)\n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "        hidden = fully_connected(\n",
    "            last_conv_layer_flat, n_hidden, activation_fn=hidden_activation,\n",
    "            weights_initializer=initializer)\n",
    "        outputs = fully_connected(\n",
    "            hidden, n_outputs, activation_fn=None,\n",
    "            weights_initializer=initializer)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                      scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "                              for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this code defines the hyper parameters of the DQN architecture. The `trainable_vars_by_name` dictionary gathers all trainable vars of the DQN. This will help us implement the copy feature later down the road.\n",
    "\n",
    "Now let's make the input placeholed, the two DQN, and the opretation to copy the critic DQN to the actor DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width,\n",
    "                                            input_channels])\n",
    "actor_q_values, actor_vars = q_network(X_state, scope=\"q_networks/actor\")\n",
    "critic_q_values, critic_vars = q_network(X_state, scope=\"q_networks/critic\")\n",
    "\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name])\n",
    "            for var_name, actor_var in actor_vars.items()]\n",
    "copy_critic_to_actor = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actor DQN can play Ms. PacMan, but you probably want to combine it with the _ε-greedy policy_ or another sort of exploration strategy.\n",
    "\n",
    "What about the critic? In short, it will learn by trying to estimate the actors Q-Values, by training againtst the _replay memory_. This will be done using supervised learning techniques. Afterwhich, the critic will be copied to the actor.\n",
    "\n",
    "Refere to book for equation.\n",
    "\n",
    "> While you ommit replay memory, it's highly recommended that you don't. Without it, the critic DQN would become very correlated, which would induce bias, and slow down convergence.\n",
    "\n",
    "Let's add the DON's training ops. First be need to be able to compute its predicted Q-Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-80432dedd0a6>:3: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "q_value = tf.reduce_sum(critic_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                        axis=1, keep_dims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, be add the training ops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "cost = tf.reduce_mean(tf.square(y - q_value))\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get the execution phase, let's build the replay memory and some other tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 10000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    indicies = rnd.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for idx in indicies:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3],\n",
    "            cols[4].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create the ε-greedy policy for the agent, that decreases ε from 1.0 to 0.05 in 50,000 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_min = 0.05\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 50000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if rnd.rand() < epsilon:\n",
    "        return rnd.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to start training! First the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10000 \n",
    "training_start = 1000\n",
    "training_interval = 3\n",
    "save_steps = 50\n",
    "copy_steps = 25\n",
    "discount_rate = 0.95\n",
    "skip_start = 90\n",
    "batch_size = 50\n",
    "iteration = 0\n",
    "checkpoint_path = \"./my_dqn.ckpt\"\n",
    "done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy.random as rnd\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if done: # game over, restart\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start): # skip start\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observation(obs)\n",
    "        \n",
    "        # actor evaluate\n",
    "        q_values = actor_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "        \n",
    "        # actor play\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render() # For fun!\n",
    "        next_state = preprocess_observation(obs)\n",
    "        \n",
    "        # save action to memory\n",
    "        replay_memory.append((state, action,reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "        \n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue\n",
    "            \n",
    "        # Critic learns\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memories(batch_size))\n",
    "        next_q_values = actor_q_values.eval(\n",
    "            feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        training_op.run(feed_dict={X_state: X_state_val,\n",
    "                                   X_action: X_action_val, y: y_val})\n",
    "        \n",
    "        # Regularly copy critic to actor\n",
    "        if step % copy_steps == 0:\n",
    "            copy_critic_to_actor.run()\n",
    "            \n",
    "        # And sove regularly!\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
