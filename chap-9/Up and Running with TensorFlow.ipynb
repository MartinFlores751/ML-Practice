{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9\n",
    "## Up and Running with Tensorflow\n",
    "Refer to the book for other libraries and why tensorflow is sooo cooool! Anywho, hello tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it! Do note, **this does not perform any computation, even though it looks like it does**, it just creates a computation graph. Even the variables are not initialized yet! To evaluate this, you need to start a tf session and use it to initialize the varables and evaulate f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this works, it's really tedious... Here's a simpler version of this!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a tensorflow program is split into two parts: the first part builds the computation graph (called the _construction phase_), and the second part runs it (called the _execution phase_).\n",
    "\n",
    "## Managing Graphs\n",
    "Anything made gets added to the default graph,but you can manage multiple independant graphs too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something very important to note is that all node values are dropped between graph runs, except variable values. Here is an efficient example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with TensorFlow\n",
    "Heres an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent\n",
    "We'll try and implement Batch Gradient Descent! First we will do this manually, then use autodiff, then use TensorFlow's out-of-the-box optimizers!\n",
    "\n",
    "**NOTE:** When performing Gradient Descent, be sure to normalize the input, otherwise training will take much longer!!!\n",
    "\n",
    "### Manually Computing the Gradients\n",
    "Here's a breif run down:\n",
    "- `random_uniform()` creates a node in the graph that will generate a tensor containing random values.\n",
    "- `assign()` creaes a node that will assign a new value to a variable\n",
    "- The main loop executes training repeatedly and every 100 iters it prints out the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  5.84069\n",
      "Epoch 100 MSE =  4.9645343\n",
      "Epoch 200 MSE =  4.9123673\n",
      "Epoch 300 MSE =  4.8822002\n",
      "Epoch 400 MSE =  4.8605113\n",
      "Epoch 500 MSE =  4.8448167\n",
      "Epoch 600 MSE =  4.833452\n",
      "Epoch 700 MSE =  4.8252196\n",
      "Epoch 800 MSE =  4.81925\n",
      "Epoch 900 MSE =  4.814919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data_plus_bias = scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff\n",
    "While it's fairly easy to do the following, it becomes a real hassle to do that every single time, plus this leads to more error prone code! We could use _symbolic differnetiation_ to automatically find the equations for the partial derivatives, but the resulting code would not be efficient.\n",
    "\n",
    "Thankfully TensorFlow can automatically and efficiently compute the gradients for us. Just replace the `gradients` code above with this:\n",
    "```python\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "```\n",
    "\n",
    "### Using an Optimizer\n",
    "It gets even easier, just swap out `gradients` and `training_op` with the following:\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "```\n",
    "If you want to use another optimizer, you just need to change one line! For example, we can use a momentum optimizer(See Chapter 11)\n",
    "```python\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)\n",
    "```\n",
    "\n",
    "## Feeding Data to the Training Algorithm\n",
    "Now we're going to implement Mini-batch Gradient Descent. To do this X and y will utilize special nodes (placeholders). To create one just use `placeholder()` and specify the type and optinally the shape.\n",
    "\n",
    "To implement Mini-batch Gradient Descent we can tweak the existing code sligtly like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring Models\n",
    "Once you trained your model, you should save it so you can come back to it whenever you want! Moreover, save friggin checkpoints when training! Saving tensorflow models is super easy, just create a `Saver` node at the end of the construction phase and call its `save()` method to save the model.\n",
    "\n",
    "```python\n",
    "[...]\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        \n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")\n",
    "```\n",
    "\n",
    "Restoring the model is easy, make a `Saver` at the end of the constuction phase just like before, and at the beginning of the execution phase, do the following `restore()` frome the `Saver` object:\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    [...]\n",
    "```\n",
    "By default `Saver` saves and restores all variables under their own name, but if more control is needed, you can specify which variables to save or restore, and what names to use! Example here\n",
    "```python\n",
    "saver = tf.train.Saver({\"weights\": theta})\n",
    "```\n",
    "\n",
    "## Visualizing the Graph and Training Curves Using TensorBoard\n",
    "\n",
    "After doin all of that magics, you're going to want to start properly logging your data! To do so, take note of the data using classic date and time stuffs as follows:\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "```\n",
    "\n",
    "After constructing your model, but before running it, add the following lines:\n",
    "\n",
    "```python\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "```\n",
    "\n",
    "Now you need to remeber to log data back every once and a while like so:\n",
    "```python\n",
    "[...]\n",
    "for batch_index in range(n_batches):\n",
    "    X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "    if batch_index % 10 == 0:\n",
    "        summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        step = epoch * n_batches + batch_index\n",
    "        file_writer.add_summary(summary_str, step)\n",
    "    sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "[...]\n",
    "```\n",
    "\n",
    "And at the end of the program, don't forget to call `file_writer.close()`!\n",
    "\n",
    "Start tensorboard by using the following:\n",
    "```bash\n",
    "$ tenorboard --logdir tf_logs/\n",
    "```\n",
    "If you want to look at it in Jupyter, call the `show_graph()` function!\n",
    "\n",
    "### Name Scopes\n",
    "To avoid clutter in the future with NN, create namescopes as follows:\n",
    "```python\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "```\n",
    "\n",
    "## Modularity\n",
    "Rather than repeat yourself if you want to create multiple instances of the same model, tensorflow lets you stay DRY by letting you create functions to build your model! Example as follows:\n",
    "\n",
    "```python\n",
    "def relu(x):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "    \n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "```\n",
    "\n",
    "To format the graph on Tensorboard even nicer, do the following and add a `name_scope()`:\n",
    "\n",
    "```python\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        [...]\n",
    "```\n",
    "\n",
    "## Sharing Variables\n",
    "One way of sharing variables in TensorFlow is to simply definite before used and pass it to every funcition that uses it. This tends to be rather tedious thought. Some people make a dicitonary of items and pass those instead. Others make classes for each module. Or you can set the shared variable of the `relu()` function as an attribute like so:\n",
    "\n",
    "```python\n",
    "def relu(x)\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        [...]\n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")\n",
    "```\n",
    "\n",
    "Another method is to use the `get_variable()` tf method like so:\n",
    "\n",
    "```python\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "```\n",
    "\n",
    "If you want to reuse the variable, you must explicitly say so:\n",
    "\n",
    "```python\n",
    "with tf.variable_scope(\"relu\", reuse=true):\n",
    "    threshold = tf.get_variable(\"threshold\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
