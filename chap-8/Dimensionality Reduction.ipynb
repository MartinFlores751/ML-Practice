{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Recution\n",
    "Typically, when we train we have instances that have trillions upon billions of features per instance. Not only does it make training really slow, it could also make it harder to find a good solution. This is often refered as the _curse of dimensionality_.\n",
    "\n",
    "Fortunatly for us, it's often possible to reduce the dimensions!!! For example, the edges of the MNIST set could be trimmed since they hold no useful information. If two neighboring pixels are highly correlated, they can be merged into a single pixel. Keep in mind that reducing features leads to information loss! This may lead to slightly worse performance. That aside, dimensionality reduction can lead to data visualization(AKA _DataViz_). Here we will look at three techniques: PCA, Kernel PCA, and LLE.\n",
    "\n",
    "## The Curse of Dimensionality\n",
    "High dimensional stuff be cray cray fam. Look at the book for diz. Keep in mind, the greater the dimensions, the greater the risk of overfitting it! One theoretical solution would be to increase the training set to a rediculous size... however this in practice is not possible.\n",
    "\n",
    "## Main Approaches for Dimensionality Reduction\n",
    "Before we look at specific reduction algos, let's look at two main approaches: projection and Manifold Learning\n",
    "\n",
    "### Projection\n",
    "In most real-world problems, training instances are _not_ spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated, hence they actually lie within a lower-dimensional _subspace_. Projection is not the best approach however. Take a look at the famous _Swiss roll_ toy dataset. Dropping a plane is bad, you need to unroll it!\n",
    "\n",
    "### Manifold Learning\n",
    "Many dimensionality reduction algos work by modeling the _manifold_ on which the training instances lie upon. This is called _Manifold Learning_. This relies onthe _manifold assumtion_, also called _manifold hypothesis_, which holds that most real-world highdimensional data lie closer to a much lower-dimensional manifold.\n",
    "\n",
    "Again, think about the MNIST set: all handwriten digets have some similarities. They have connected lines, white borders, and are more or less centered. If we had instead randomly generated images, we probably wouldn't be able to reduce the dimensions, given that constraints tend to lead to a dataset being projected onto a lower dimension manifold.\n",
    "\n",
    "The manifold assumption also has another assumption being that the task at hand will be simpler if expressed in a lower dimension. In short, it all depends on how the data set is!\n",
    "\n",
    "## PCA\n",
    "_Principal Component Analysis_(PCA) is the most popular dimensionality reduction algo. It identifies the hyperplane that lies closest to the data, then projects the data onto it.\n",
    "\n",
    "### Preserving the Variance\n",
    "You need to choose the right plane and axis to maintain the variance! This is to minizmize the mean squared distance between the original and it's projection onto the axis. This is the idea behind PCA.\n",
    "\n",
    "### Principal Components\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the set. It then recursivly finds axis orthoganl to the previous axises. The unit vector that describes the ith axis is called the ith _principal component_. So how do you find the PCs of the set? You do so using a technique called _Singular Value Decomposition_(SVD). That decomposes the trainng set X to the three matricies U E and V^T which contains all of the principal components. Like the following..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to generate a dataset for X first!\n",
    "\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:, 0]\n",
    "c2 = V.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting Down to d Dimensions\n",
    "Once all principal components have be identified, you can reduce the dimensionality of the dataset down to _d_ dimensions by projecting it onto the first _d_ principal components. To project the set onto the hyperplane, simply do the dot maxtrix of the set and the matrix W_d_, defined by the first _d_ principal components. The following code does just that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there's the Scikit version of doing things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decompositions import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio\n",
    "Another useful piece of information, the _explained variance ratio_ of each principal component. It indicates the portion of the dataset's variance that lies along the axis of each principal component. Here's a sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions\n",
    "Instead of arbitraraly choosing the number of dimensions to reduce down to, it's best to choose a number that adds up to a sufficently large portion of variance (95%)... unless it's for data visualization.\n",
    "\n",
    "The following finds a _d_ that maintaince a certain amount of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there's a more direct way to get the ratio you desire..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to find the optimal number of dimesions is to plot the darn thing and find the dimention size that you want!\n",
    "\n",
    "### PCA for Compression\n",
    "By applying PCA to the dataset, you can achieve resonable compression. For example the MNIST dataset could be compressed by 20%! It's possible to recover the reduced dataset, but there will be some information loss. THe following is an example of PCA to 154 components then recovering the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)\n",
    "X_mnist_reduced = pca.fit_transform(X_mnist)\n",
    "X_mnist_recovered = pca.inverse_transform(X_mnist_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
