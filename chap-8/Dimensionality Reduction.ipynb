{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Recution\n",
    "Typically, when we train we have instances that have trillions upon billions of features per instance. Not only does it make training really slow, it could also make it harder to find a good solution. This is often refered as the _curse of dimensionality_.\n",
    "\n",
    "Fortunatly for us, it's often possible to reduce the dimensions!!! For example, the edges of the MNIST set could be trimmed since they hold no useful information. If two neighboring pixels are highly correlated, they can be merged into a single pixel. Keep in mind that reducing features leads to information loss! This may lead to slightly worse performance. That aside, dimensionality reduction can lead to data visualization(AKA _DataViz_). Here we will look at three techniques: PCA, Kernel PCA, and LLE.\n",
    "\n",
    "## The Curse of Dimensionality\n",
    "High dimensional stuff be cray cray fam. Look at the book for diz. Keep in mind, the greater the dimensions, the greater the risk of overfitting it! One theoretical solution would be to increase the training set to a rediculous size... however this in practice is not possible.\n",
    "\n",
    "## Main Approaches for Dimensionality Reduction\n",
    "Before we look at specific reduction algos, let's look at two main approaches: projection and Manifold Learning\n",
    "\n",
    "### Projection\n",
    "In most real-world problems, training instances are _not_ spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated, hence they actually lie within a lower-dimensional _subspace_. Projection is not the best approach however. Take a look at the famous _Swiss roll_ toy dataset. Dropping a plane is bad, you need to unroll it!\n",
    "\n",
    "### Manifold Learning\n",
    "Many dimensionality reduction algos work by modeling the _manifold_ on which the training instances lie upon. This is called _Manifold Learning_. This relies onthe _manifold assumtion_, also called _manifold hypothesis_, which holds that most real-world highdimensional data lie closer to a much lower-dimensional manifold.\n",
    "\n",
    "Again, think about the MNIST set: all handwriten digets have some similarities. They have connected lines, white borders, and are more or less centered. If we had instead randomly generated images, we probably wouldn't be able to reduce the dimensions, given that constraints tend to lead to a dataset being projected onto a lower dimension manifold.\n",
    "\n",
    "The manifold assumption also has another assumption being that the task at hand will be simpler if expressed in a lower dimension. In short, it all depends on how the data set is!\n",
    "\n",
    "## PCA\n",
    "_Principal Component Analysis_(PCA) is the most popular dimensionality reduction algo. It identifies the hyperplane that lies closest to the data, then projects the data onto it.\n",
    "\n",
    "### Preserving the Variance\n",
    "You need to choose the right plane and axis to maintain the variance! This is to minizmize the mean squared distance between the original and it's projection onto the axis. This is the idea behind PCA.\n",
    "\n",
    "### Principal Components\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the set. It then recursivly finds axis orthoganl to the previous axises. The unit vector that describes the ith axis is called the ith _principal component_. So how do you find the PCs of the set? You do so using a technique called _Singular Value Decomposition_(SVD). That decomposes the trainng set X to the three matricies U E and V^T which contains all of the principal components. Like the following..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to generate a dataset for X first!\n",
    "\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:, 0]\n",
    "c2 = V.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting Down to d Dimensions\n",
    "Once all principal components have be identified, you can reduce the dimensionality of the dataset down to _d_ dimensions by projecting it onto the first _d_ principal components. To project the set onto the hyperplane, simply do the dot maxtrix of the set and the matrix W_d_, defined by the first _d_ principal components. The following code does just that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there's the Scikit version of doing things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decompositions import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio\n",
    "Another useful piece of information, the _explained variance ratio_ of each principal component. It indicates the portion of the dataset's variance that lies along the axis of each principal component. Here's a sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions\n",
    "Instead of arbitraraly choosing the number of dimensions to reduce down to, it's best to choose a number that adds up to a sufficently large portion of variance (95%)... unless it's for data visualization.\n",
    "\n",
    "The following finds a _d_ that maintaince a certain amount of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there's a more direct way to get the ratio you desire..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to find the optimal number of dimesions is to plot the darn thing and find the dimention size that you want!\n",
    "\n",
    "### PCA for Compression\n",
    "By applying PCA to the dataset, you can achieve resonable compression. For example the MNIST dataset could be compressed by 20%! It's possible to recover the reduced dataset, but there will be some information loss. THe following is an example of PCA to 154 components then recovering the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)\n",
    "X_mnist_reduced = pca.fit_transform(X_mnist)\n",
    "X_mnist_recovered = pca.inverse_transform(X_mnist_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA\n",
    "One problem of PCA is that it requires the whole set to fit in memory! Luckly, there _Incremental PCA_(IPCA) is a thing to split the set into mini-batches and feed it to the algo one batch at a time. This can also be used for online algos!\n",
    "\n",
    "Here's an example using the MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = InvcrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_mnist, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_mnist_reduced = inc_pca.transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can instead read in data from memory only when it needs it such as the following..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized PCA\n",
    "This is a stochastic algo that finds an approximation of the first _d_ principal components. It's complexity is:\n",
    "O(m * d^2) + O(d^3) instead of the same, swapping _d_ for _n_. Hence, when the improvment in speed is massive when _d_ is much smaller than _n_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced rnd_pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA\n",
    "We can apply the Kernel trick discussed in chapter 5 to perform complex nonlinear projections for dimensionality reduction. It's good at perversing clusters of instances and sometimes even unrolling datasets that lie close to a twisted manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Kernel and Tuning Hyperparameters\n",
    "Since kPCA is an unsupervised algo, there's no obvious performance measure for it. However, since this is often a preparation step for a supervised learning task you can use grid search to find the best kernel and hyperparameters. Here's a example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "gird_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach, this time entierly unsupervised, is to choose the kernel with the least reconstruction error. This however is not as easy as linear PCA... take a look at the swiss roll image on 220. Since the feature map essentially has infinite dimensions, it can only map an approximation. Big oof.\n",
    "\n",
    "As for how to perform reconstruction, one solution is to train a supervised regression model, with  projected instances as the training set and the original instances as the targets. Scikit can do this automagically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then compute the error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE\n",
    "_Locally Linear Embedding_(LLE) is another _nonlinear dimensionality reduction_(NLDR) technique. It's a Manifold Learning technique that doesn't rely on projections like the previous ones!!! It works by first measuring how each training instance linearly relates to its closest neighbors and then lookikng for a low-dimensional representation of the set in which the local relations are best preserved. This makes it particularly good at unrolling these swiss rolls.\n",
    "\n",
    "In the following example we use Scikit's `LocallyLinearEmbedding` class to unroll the swiss roll! Note that distances are _not_ preserved on a larger scale: the left part of the roll is squeezed while the right part is stretched!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the book for the implementation and the computational complexity, just know that this doesn't scale well with very large datasets!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Dimensionality Reduction Techniques\n",
    "- _Multidimensional Scaling_(MDS) reduces dimensionality while trying to preserve the distances between instances\n",
    "- _Isomap_ Creates a map by connecting it's nearest neighbors, then reduces dimensionality while trying to preserve the _geometric distances_ between instances.\n",
    "- _t-Distributed Stochastic Neighbor Embedding_ (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. Mostly used for visualization, in partucular to visualize clusters of instances in high dimensional space.\n",
    "- _Linear Discriminant Analysis_(LDA) is actually a classification algo, but during training, it learns the most discriminative axes between the classes, and these axes can be used to define a hyperplane onto which to project the data. The benefit is that this projection will keep the classes as far as possible, so LDA is a good technique to reduce dimensionality before running another classification algo such as a SVM classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
